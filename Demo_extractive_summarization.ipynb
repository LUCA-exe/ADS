{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ul2h8VSfHcWV",
        "O_24_FB4Play"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **THExtended: Transformer-based Highlights Extraction for News Summarization**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Authors:** Alessio Paone, Flavio Spuri, Luca Agnese, Luca Zilli"
      ],
      "metadata": {
        "id": "6kJ_1ZLjBo-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebok to use the **THExtended** summarization method\n",
        "\n",
        "This **Demo** provides the opportunity to execute the code cells presented below for test the articles.\n",
        "To set up the scenarios, it is recommended to begin by running the ***Prepare the Repository and Dependencies*** code cells. Running the mentioned configurations is necessary to ensure the successful execution of the prepared phase.\n",
        "\n",
        "A brief recap of the project key definitions:\n",
        "\n",
        "* **Summarization** has received considerable attention in the field of Natural Language Processing (NLP) in recent years. It has been widely applied in various domains, including news summarization and extracting important sections from scientific papers, such as highlights.\n",
        "* **Extractive summarization** is a specific approach to summarization where the goal is to select and extract the most important sentences or phrases from a given document or text corpus."
      ],
      "metadata": {
        "id": "n2iLKW0xCxtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prepare the Repository and Dependencies**\n",
        "\n",
        "This section is provided for downloading the repository, all the necessary dependencies and the dataset samples from [HuggingFace](https://huggingface.co/datasets/cnn_dailymail)."
      ],
      "metadata": {
        "id": "ul2h8VSfHcWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the repository from github\n",
        "%%capture\n",
        "!git clone https://github.com/Raffix-14/THExtended_.git"
      ],
      "metadata": {
        "id": "3LpGjy9yI19h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASELLA DA TOGLIERE PRIMA DELLA CONSEGNA - USATA SOLO PER DEBUG (models folder e gia dentro il github di pao)\n",
        "!mkdir THExtended_/models"
      ],
      "metadata": {
        "id": "ADcYcKvtOA9t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CASELLA DA TOGLIERE PRIMA DELLA CONSEGNA - USATA SOLO PER DEBUG (scaricare wheights drive drive)\n",
        "!cp /content/drive/MyDrive/THExtEnded/alpha_075/model/checkpoint-5972 -r THExtended_/models"
      ],
      "metadata": {
        "id": "c5t9WN_JMSbo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uX1_gVzf_Jeq"
      },
      "outputs": [],
      "source": [
        "# Install all the requested requirements\n",
        "%%capture\n",
        "!pip install rouge\n",
        "!pip install transformers datasets accelerate nvidia-ml-py3 sentencepiece evaluate\n",
        "!pip install bert_score\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the 'cnn_dailymail' dataset from HuggingFace\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"cnn_dailymail\", '3.0.0')"
      ],
      "metadata": {
        "id": "VIuAev3kO8H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Run the highlights extraction**\n",
        "\n",
        "This section is provided for extract the **highlights** from an article example contained in **CNN/DailyMail**.\n",
        "\n",
        "The following cells contain respectively:\n",
        "\n",
        "\n",
        "1.   **Load and show** a single article from the dataset\n",
        "2.   Set up the model and load the **fine-tuned weights**\n",
        "3.   Declare few functions to process the article **corpus** and **context**\n",
        "4.   Finally use our model to **perform the extraction** and visualize the results\n",
        "5.   Final **comparison** between the extracted sentences and the golden highlights\n",
        "\n",
        "**Note:** Take into account the the dataset used in our work (and in this very notebook) predominantly contains **abstractive** highlights\n"
      ],
      "metadata": {
        "id": "O_24_FB4Play"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the first article contained in the test split of the dataset\n",
        "article_corpus = dataset['test'][0]['article']\n",
        "article_summary = dataset['test'][0]['highlights']\n",
        "print(f\"The article text corpus contains: \\n\\n{article_corpus}\\n\\n\")\n",
        "print(f\"The 'gold' higlights are: \\n\\n{article_summary}\")"
      ],
      "metadata": {
        "id": "NF4AuAx0QnlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import math\n",
        "import spacy\n",
        "\n",
        "# Set up the model and the tokenizer (use our best fine-tuned weights) and prepare the model for the test phase\n",
        "\n",
        "model_name = '/content/THExtended_/models/checkpoint-5972'\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "similarity_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model.to(torch.device(\"cuda:0\"))"
      ],
      "metadata": {
        "id": "6R0DcKHrUFPQ"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHIEDERE A PAO!: Il summary ritornato non e splittato (togliere poi il commento prima di consegnaer!!!)\n",
        "# Declare 'modified' versions (shorter snippets of code) from our project functions and prepare the article processing\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg') # Smart 'sentence splitter'\n",
        "\n",
        "def split_sentence(text):\n",
        "        return [s.text.strip() for s in nlp(text).sents]  # Splitting into sentences and cleaning\n",
        "\n",
        "def extract_context(article_sentences):\n",
        "        # Compute the index for the first section\n",
        "        cut_off = math.ceil(len(article_sentences) / 3)\n",
        "        return ' '.join(sent for sent in article_sentences[:cut_off])\n",
        "\n",
        "def process_row(row):\n",
        "        # Unpack the parameter tuple\n",
        "        article, summary = row\n",
        "        article_sentences = split_sentence(article)\n",
        "        context = extract_context(article_sentences)\n",
        "        # Return the processed 'article' object\n",
        "        return {\"sentences\": article_sentences, \"context\": context, \"highlights\": summary}"
      ],
      "metadata": {
        "id": "RdPppPyJXuwq"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the needed function from our repository and visualize the highlights\n",
        "from THExtended_.utils import get_scores\n",
        "\n",
        "processed_data = process_row((article_corpus, article_summary))\n",
        "golden_highlights = processed_data[\"highlights\"].split(\".\")\n",
        "#num_highlights = len(golden_highlights)\n",
        "num_highlights = 2 # The first article contains just 2 highlights\n",
        "article_sentences, context = processed_data[\"sentences\"], processed_data[\"context\"]\n",
        "ranked_sents, ranked_scores = get_scores(article_sentences, context, model, tokenizer)\n",
        "print(f\"The first {num_highlights} extracted sentences are: \\n\")\n",
        "\n",
        "for sentence, score in zip(ranked_sents[:num_highlights], ranked_scores[:num_highlights]):\n",
        "  print(f\"Sentence > {sentence}\")\n",
        "  print(f\"Model score > {round(score, 5)}\\n\")"
      ],
      "metadata": {
        "id": "5lJsGw_uWCoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the 'golden' highlights with the extracted one\n",
        "print(\"The golden highlights are:\\n\")\n",
        "for idx, real_s in enumerate(golden_highlights[:num_highlights]):\n",
        "  print(f\"{idx+1}. {real_s.strip()}\")\n",
        "\n",
        "print(\"\\nThe predicted highlights are:\\n\")\n",
        "for idx, pred_s in enumerate(ranked_sents[:num_highlights]):\n",
        "  print(f\"{idx+1}. {pred_s.strip()}\")"
      ],
      "metadata": {
        "id": "SbgFlwTOhWw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KEPT JUST TO REMOVE BEFORE SUBMISSION !!! - Run the test from a pretrained model checkpoint (best version)\n",
        "!python3 /content/test.py \\\n",
        "--dataset_path=/content/content/cnn_dm_10k \\\n",
        "--save_dataset_on_disk=0  \\\n",
        "--output_dir=/content/output \\\n",
        "--train_batch_size=64 \\\n",
        "--gradient_accumulation_steps=2 \\\n",
        "--num_train_example=10000 \\\n",
        "--num_val_example=1500 \\\n",
        "--num_test_examples=1500 \\\n",
        "--alpha=1.0 \\\n",
        "--model_name_or_path=/content/10k_a1_2epoch/train/model/checkpoint-5972"
      ],
      "metadata": {
        "id": "HnN4zi8Ku2ZS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}